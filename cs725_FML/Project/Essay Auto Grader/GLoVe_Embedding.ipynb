{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac2f4c9",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:24.455985Z",
          "iopub.status.busy": "2022-11-24T19:11:24.455463Z",
          "iopub.status.idle": "2022-11-24T19:11:24.470125Z",
          "shell.execute_reply": "2022-11-24T19:11:24.468484Z"
        },
        "papermill": {
          "duration": 0.028604,
          "end_time": "2022-11-24T19:11:24.473078",
          "exception": false,
          "start_time": "2022-11-24T19:11:24.444474",
          "status": "completed"
        },
        "tags": [],
        "id": "dac2f4c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2436e39b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:24.490575Z",
          "iopub.status.busy": "2022-11-24T19:11:24.489893Z",
          "iopub.status.idle": "2022-11-24T19:11:53.954210Z",
          "shell.execute_reply": "2022-11-24T19:11:53.952443Z"
        },
        "papermill": {
          "duration": 29.476581,
          "end_time": "2022-11-24T19:11:53.957519",
          "exception": false,
          "start_time": "2022-11-24T19:11:24.480938",
          "status": "completed"
        },
        "tags": [],
        "id": "2436e39b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from numpy import mean\n",
        "import statistics\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten, Bidirectional\n",
        "from tensorflow.keras.models import Sequential, load_model, model_from_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "987fe4f4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:53.977500Z",
          "iopub.status.busy": "2022-11-24T19:11:53.976339Z",
          "iopub.status.idle": "2022-11-24T19:11:54.279043Z",
          "shell.execute_reply": "2022-11-24T19:11:54.278103Z"
        },
        "papermill": {
          "duration": 0.31612,
          "end_time": "2022-11-24T19:11:54.282482",
          "exception": false,
          "start_time": "2022-11-24T19:11:53.966362",
          "status": "completed"
        },
        "tags": [],
        "id": "987fe4f4"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv(\"train.csv\")\n",
        "features = pd.read_csv(\"data1.csv\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c99dc27e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.301036Z",
          "iopub.status.busy": "2022-11-24T19:11:54.300565Z",
          "iopub.status.idle": "2022-11-24T19:11:54.320483Z",
          "shell.execute_reply": "2022-11-24T19:11:54.319502Z"
        },
        "papermill": {
          "duration": 0.031978,
          "end_time": "2022-11-24T19:11:54.323019",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.291041",
          "status": "completed"
        },
        "tags": [],
        "id": "c99dc27e"
      },
      "outputs": [],
      "source": [
        "X1_train = X.loc[:int(len(X.index)*0.8)]\n",
        "y_train = X1_train[['syntax','cohesion','vocabulary','phraseology','grammar','conventions']]\n",
        "X1_val = X.loc[int(len(X.index)*0.8)+1:int(len(X.index)*0.9)]\n",
        "y_val = X1_val[['syntax','cohesion','vocabulary','phraseology','grammar','conventions']]\n",
        "X1_test = X.loc[int(len(X.index)*0.9)+1:]\n",
        "y_test = X1_test[['syntax','cohesion','vocabulary','phraseology','grammar','conventions']] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1f31e546",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.341662Z",
          "iopub.status.busy": "2022-11-24T19:11:54.340945Z",
          "iopub.status.idle": "2022-11-24T19:11:54.356408Z",
          "shell.execute_reply": "2022-11-24T19:11:54.355043Z"
        },
        "papermill": {
          "duration": 0.028392,
          "end_time": "2022-11-24T19:11:54.359329",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.330937",
          "status": "completed"
        },
        "tags": [],
        "id": "1f31e546"
      },
      "outputs": [],
      "source": [
        "IP=14\n",
        "\n",
        "y1_train=y_train[\"syntax\"]\n",
        "y2_train=y_train[\"cohesion\"]\n",
        "y3_train=y_train[\"vocabulary\"]\n",
        "y4_train=y_train[\"phraseology\"]\n",
        "y5_train=y_train[\"grammar\"]\n",
        "y6_train=y_train[\"conventions\"]\n",
        "\n",
        "y1_val=y_val[\"syntax\"]\n",
        "y2_val=y_val[\"cohesion\"]\n",
        "y3_val=y_val[\"vocabulary\"]\n",
        "y4_val=y_val[\"phraseology\"]\n",
        "y5_val=y_val[\"grammar\"]\n",
        "y6_val=y_val[\"conventions\"]\n",
        "\n",
        "y1_test=y_test[\"syntax\"]\n",
        "y2_test=y_test[\"cohesion\"]\n",
        "y3_test=y_test[\"vocabulary\"]\n",
        "y4_test=y_test[\"phraseology\"]\n",
        "y5_test=y_test[\"grammar\"]\n",
        "y6_test=y_test[\"conventions\"]\n",
        "\n",
        "\n",
        "y1_train = np.asarray(y1_train)\n",
        "y1_val = np.asarray(y1_val)  \n",
        "\n",
        "y2_train = np.asarray(y2_train)\n",
        "y2_val = np.asarray(y2_val)  \n",
        "\n",
        "y3_train = np.asarray(y3_train)\n",
        "y3_val = np.asarray(y3_val)  \n",
        "\n",
        "y4_train = np.asarray(y4_train)\n",
        "y4_val = np.asarray(y4_val)  \n",
        "\n",
        "y5_train = np.asarray(y5_train)\n",
        "y5_val = np.asarray(y5_val)  \n",
        "\n",
        "y6_train = np.asarray(y6_train)\n",
        "y6_val = np.asarray(y6_val)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b173f26a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.379728Z",
          "iopub.status.busy": "2022-11-24T19:11:54.379254Z",
          "iopub.status.idle": "2022-11-24T19:11:54.385202Z",
          "shell.execute_reply": "2022-11-24T19:11:54.383661Z"
        },
        "papermill": {
          "duration": 0.018174,
          "end_time": "2022-11-24T19:11:54.387645",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.369471",
          "status": "completed"
        },
        "tags": [],
        "id": "b173f26a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ab634b-bfb6-4b83-ef5e-8235f2b68a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(391, 300)\n",
            "(391, 300)\n",
            "(3129, 300)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "features_array = np.asarray(features)\n",
        "features_array_train = features_array[:3129]\n",
        "features_array_val = features_array[3129:3129+391]\n",
        "features_array_test = features_array[3129+391:]\n",
        "print(features_array_test.shape)\n",
        "print(features_array_val.shape)\n",
        "print(features_array_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "76ba134b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.407009Z",
          "iopub.status.busy": "2022-11-24T19:11:54.405935Z",
          "iopub.status.idle": "2022-11-24T19:11:54.412021Z",
          "shell.execute_reply": "2022-11-24T19:11:54.411082Z"
        },
        "papermill": {
          "duration": 0.018267,
          "end_time": "2022-11-24T19:11:54.414286",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.396019",
          "status": "completed"
        },
        "tags": [],
        "id": "76ba134b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad53385-50a1-47e7-91db-cc7091f7688a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.29942881500426255\n"
          ]
        }
      ],
      "source": [
        "lr1 = LinearRegression()\n",
        "lr2 = LinearRegression()\n",
        "lr3 = LinearRegression()\n",
        "lr4 = LinearRegression()\n",
        "lr5 = LinearRegression()\n",
        "lr6 = LinearRegression()\n",
        "\n",
        "lr1.fit(features_array_train, y1_train)\n",
        "lr2.fit(features_array_train, y2_train)\n",
        "lr3.fit(features_array_train, y3_train)\n",
        "lr4.fit(features_array_train, y4_train)\n",
        "lr5.fit(features_array_train, y5_train)\n",
        "lr6.fit(features_array_train, y6_train)\n",
        "\n",
        "\n",
        "y1_pred = lr1.predict(features_array_test)\n",
        "y2_pred = lr2.predict(features_array_test)\n",
        "y3_pred = lr3.predict(features_array_test)\n",
        "y4_pred = lr4.predict(features_array_test)\n",
        "y5_pred = lr5.predict(features_array_test)\n",
        "y6_pred = lr6.predict(features_array_test)\n",
        "\n",
        "y1_pred = [round(r,1) for r in y1_pred]\n",
        "y2_pred = [round(r,1) for r in y2_pred]\n",
        "y3_pred = [round(r,1) for r in y3_pred]\n",
        "y4_pred = [round(r,1) for r in y4_pred]\n",
        "y5_pred = [round(r,1) for r in y5_pred]\n",
        "y6_pred = [round(r,1) for r in y6_pred]\n",
        "\n",
        "\n",
        "values1 = mean_squared_error(y1_pred, y1_test)\n",
        "values2 = mean_squared_error(y2_pred, y2_test)\n",
        "values3 = mean_squared_error(y3_pred, y3_test)\n",
        "values4 = mean_squared_error(y4_pred, y4_test)\n",
        "values5 = mean_squared_error(y5_pred, y5_test)\n",
        "values6 = mean_squared_error(y6_pred, y6_test)\n",
        "\n",
        "\n",
        "l=[(values1,values2,values3,values4,values5,values6)]\n",
        "print(mean(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0d067d78",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.431832Z",
          "iopub.status.busy": "2022-11-24T19:11:54.431208Z",
          "iopub.status.idle": "2022-11-24T19:11:54.439418Z",
          "shell.execute_reply": "2022-11-24T19:11:54.438365Z"
        },
        "papermill": {
          "duration": 0.019546,
          "end_time": "2022-11-24T19:11:54.441680",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.422134",
          "status": "completed"
        },
        "tags": [],
        "id": "0d067d78"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "13c221ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.459724Z",
          "iopub.status.busy": "2022-11-24T19:11:54.459087Z",
          "iopub.status.idle": "2022-11-24T19:11:54.464867Z",
          "shell.execute_reply": "2022-11-24T19:11:54.463990Z"
        },
        "papermill": {
          "duration": 0.017703,
          "end_time": "2022-11-24T19:11:54.467297",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.449594",
          "status": "completed"
        },
        "tags": [],
        "id": "13c221ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbe38b7-fbb1-4781-9fed-74797f31c353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3129, 1, 300)\n",
            "(391, 1, 300)\n",
            "(391, 1, 300)\n"
          ]
        }
      ],
      "source": [
        "lstm_embed_train = []\n",
        "for i in range(0,len(features_array_train)):\n",
        "    lstm_embed_train.append(np.asarray([features_array_train[i]]))\n",
        "    \n",
        "\n",
        "lstm_embed_test = []\n",
        "for i in range(0,len(features_array_test)):\n",
        "    lstm_embed_test.append(np.asarray([features_array_test[i]]))\n",
        "     \n",
        "\n",
        "\n",
        "lstm_embed_val = []\n",
        "for i in range(0,len(features_array_val)):\n",
        "    lstm_embed_val.append(np.asarray([features_array_val[i]]))\n",
        "    \n",
        "\n",
        "lstm_embed_test = np.asarray(lstm_embed_test)\n",
        "lstm_embed_train = np.asarray(lstm_embed_train)\n",
        "lstm_embed_val = np.asarray(lstm_embed_val)\n",
        "print(lstm_embed_train.shape)\n",
        "print(lstm_embed_val.shape)\n",
        "print(lstm_embed_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "1a435f32",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.485450Z",
          "iopub.status.busy": "2022-11-24T19:11:54.484777Z",
          "iopub.status.idle": "2022-11-24T19:11:54.491362Z",
          "shell.execute_reply": "2022-11-24T19:11:54.490455Z"
        },
        "papermill": {
          "duration": 0.018335,
          "end_time": "2022-11-24T19:11:54.493691",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.475356",
          "status": "completed"
        },
        "tags": [],
        "id": "1a435f32"
      },
      "outputs": [],
      "source": [
        "def get_model_lstm():\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=[1,300], activation = 'relu',return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='tanh'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "    model.summary()\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "ee4e0ec3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.511079Z",
          "iopub.status.busy": "2022-11-24T19:11:54.510647Z",
          "iopub.status.idle": "2022-11-24T19:11:54.516670Z",
          "shell.execute_reply": "2022-11-24T19:11:54.515424Z"
        },
        "papermill": {
          "duration": 0.017391,
          "end_time": "2022-11-24T19:11:54.518844",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.501453",
          "status": "completed"
        },
        "tags": [],
        "id": "ee4e0ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c827530-3b65-4ffe-b323-059a4b9b1a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 1, 128)            219648    \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 1, 32)             4128      \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 1, 32)             0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,809\n",
            "Trainable params: 223,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_13 (LSTM)              (None, 1, 128)            219648    \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 1, 32)             4128      \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 1, 32)             0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,809\n",
            "Trainable params: 223,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_14 (LSTM)              (None, 1, 128)            219648    \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 1, 32)             4128      \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 1, 32)             0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,809\n",
            "Trainable params: 223,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_15 (LSTM)              (None, 1, 128)            219648    \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 1, 32)             4128      \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 1, 32)             0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,809\n",
            "Trainable params: 223,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_16 (LSTM)              (None, 1, 128)            219648    \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 1, 32)             4128      \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 1, 32)             0         \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,809\n",
            "Trainable params: 223,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_17 (LSTM)              (None, 1, 128)            219648    \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 1, 32)             4128      \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 1, 32)             0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 223,809\n",
            "Trainable params: 223,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 3s 11ms/step - loss: 0.9344 - val_loss: 0.4518\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5779 - val_loss: 0.5042\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5580 - val_loss: 0.4345\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5363 - val_loss: 0.5716\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5265 - val_loss: 0.4940\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5205 - val_loss: 0.4493\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5180 - val_loss: 0.5512\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5200 - val_loss: 0.5304\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5109 - val_loss: 0.4673\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5131 - val_loss: 0.4356\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5057 - val_loss: 0.4443\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5086 - val_loss: 0.4276\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5015 - val_loss: 0.4364\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5010 - val_loss: 0.4510\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4968 - val_loss: 0.4276\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5050 - val_loss: 0.4327\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4966 - val_loss: 0.4630\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4993 - val_loss: 0.4349\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4885 - val_loss: 0.4935\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4919 - val_loss: 0.4398\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 0.4857 - val_loss: 0.4612\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 0.4861 - val_loss: 0.4280\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4877 - val_loss: 0.4269\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4827 - val_loss: 0.4274\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4880 - val_loss: 0.4272\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 0.4839 - val_loss: 0.4295\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 0.4822 - val_loss: 0.4297\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 0.4816 - val_loss: 0.4430\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 0.4783 - val_loss: 0.4293\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4832 - val_loss: 0.4269\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4767 - val_loss: 0.4460\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4814 - val_loss: 0.4274\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4749 - val_loss: 0.4412\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4754 - val_loss: 0.4286\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4738 - val_loss: 0.4271\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4762 - val_loss: 0.4288\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4695 - val_loss: 0.4470\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4719 - val_loss: 0.4282\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4714 - val_loss: 0.4306\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4675 - val_loss: 0.4306\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4687 - val_loss: 0.4494\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4701 - val_loss: 0.4292\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4672 - val_loss: 0.4580\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4659 - val_loss: 0.4285\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4642 - val_loss: 0.4379\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4644 - val_loss: 0.4402\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4662 - val_loss: 0.4376\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4617 - val_loss: 0.4331\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4619 - val_loss: 0.4289\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4597 - val_loss: 0.4270\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4643 - val_loss: 0.4281\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4612 - val_loss: 0.4369\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4602 - val_loss: 0.4286\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4584 - val_loss: 0.4390\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4599 - val_loss: 0.4411\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4553 - val_loss: 0.4270\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4540 - val_loss: 0.4405\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4579 - val_loss: 0.4358\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4540 - val_loss: 0.4266\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4532 - val_loss: 0.4428\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4523 - val_loss: 0.4266\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4541 - val_loss: 0.4270\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4534 - val_loss: 0.4266\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4546 - val_loss: 0.4305\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4534 - val_loss: 0.4291\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4511 - val_loss: 0.4271\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4519 - val_loss: 0.4274\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4501 - val_loss: 0.4284\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4515 - val_loss: 0.4266\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4504 - val_loss: 0.4271\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4513 - val_loss: 0.4330\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4508 - val_loss: 0.4388\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4471 - val_loss: 0.4349\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4480 - val_loss: 0.4272\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4478 - val_loss: 0.4266\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4471 - val_loss: 0.4355\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4472 - val_loss: 0.4310\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4461 - val_loss: 0.4268\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4463 - val_loss: 0.4301\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4455 - val_loss: 0.4294\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4460 - val_loss: 0.4334\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4455 - val_loss: 0.4293\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4465 - val_loss: 0.4350\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4453 - val_loss: 0.4283\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4439 - val_loss: 0.4361\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4433 - val_loss: 0.4279\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4470 - val_loss: 0.4313\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4454 - val_loss: 0.4334\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4434 - val_loss: 0.4300\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4433 - val_loss: 0.4266\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4442 - val_loss: 0.4280\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4431 - val_loss: 0.4272\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4407 - val_loss: 0.4419\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4436 - val_loss: 0.4320\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4417 - val_loss: 0.4297\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4411 - val_loss: 0.4286\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4416 - val_loss: 0.4359\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4417 - val_loss: 0.4348\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4398 - val_loss: 0.4268\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4407 - val_loss: 0.4336\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 3s 10ms/step - loss: 1.0649 - val_loss: 0.5155\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.6023 - val_loss: 0.5073\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5944 - val_loss: 0.5498\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5766 - val_loss: 0.4785\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5697 - val_loss: 0.4736\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5630 - val_loss: 0.5034\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5489 - val_loss: 0.5338\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5519 - val_loss: 0.4989\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5402 - val_loss: 0.5080\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5401 - val_loss: 0.4706\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5397 - val_loss: 0.4829\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5357 - val_loss: 0.4839\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5285 - val_loss: 0.5186\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5271 - val_loss: 0.4748\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5305 - val_loss: 0.4679\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5252 - val_loss: 0.4805\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5218 - val_loss: 0.4698\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5218 - val_loss: 0.4705\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5186 - val_loss: 0.5918\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5170 - val_loss: 0.4864\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5166 - val_loss: 0.5101\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5182 - val_loss: 0.5361\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5157 - val_loss: 0.5129\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5162 - val_loss: 0.4707\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5099 - val_loss: 0.5145\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5110 - val_loss: 0.4756\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5100 - val_loss: 0.4674\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5059 - val_loss: 0.4674\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5058 - val_loss: 0.4677\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5009 - val_loss: 0.4706\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5044 - val_loss: 0.5864\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5027 - val_loss: 0.4675\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4994 - val_loss: 0.4688\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4996 - val_loss: 0.4729\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4998 - val_loss: 0.4734\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4992 - val_loss: 0.4688\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4948 - val_loss: 0.4684\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4958 - val_loss: 0.4845\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4909 - val_loss: 0.4749\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4899 - val_loss: 0.4693\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4890 - val_loss: 0.4716\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4914 - val_loss: 0.4681\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4883 - val_loss: 0.4674\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4886 - val_loss: 0.4992\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4895 - val_loss: 0.4680\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4859 - val_loss: 0.4677\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4875 - val_loss: 0.4674\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4874 - val_loss: 0.4683\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4851 - val_loss: 0.4689\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4824 - val_loss: 0.4673\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4828 - val_loss: 0.4710\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4820 - val_loss: 0.4681\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4787 - val_loss: 0.4768\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4811 - val_loss: 0.4682\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4798 - val_loss: 0.4684\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4776 - val_loss: 0.4684\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4810 - val_loss: 0.4695\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4796 - val_loss: 0.4713\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4780 - val_loss: 0.4674\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4791 - val_loss: 0.4699\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 0.4785 - val_loss: 0.4679\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.4766 - val_loss: 0.4681\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.4755 - val_loss: 0.4692\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 0.4777 - val_loss: 0.4690\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4743 - val_loss: 0.4678\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4767 - val_loss: 0.4705\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4734 - val_loss: 0.4675\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4743 - val_loss: 0.4698\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4737 - val_loss: 0.4718\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4724 - val_loss: 0.4682\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4708 - val_loss: 0.4674\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4704 - val_loss: 0.4674\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4720 - val_loss: 0.4684\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4695 - val_loss: 0.4683\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4750 - val_loss: 0.4781\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4721 - val_loss: 0.4678\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4705 - val_loss: 0.4674\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4717 - val_loss: 0.4755\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4713 - val_loss: 0.4679\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4704 - val_loss: 0.4675\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4682 - val_loss: 0.4713\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4668 - val_loss: 0.4742\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4670 - val_loss: 0.4679\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4686 - val_loss: 0.4833\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4696 - val_loss: 0.4742\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4670 - val_loss: 0.4737\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4676 - val_loss: 0.4673\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4670 - val_loss: 0.4675\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4671 - val_loss: 0.4674\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4661 - val_loss: 0.4676\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4670 - val_loss: 0.4673\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4669 - val_loss: 0.4675\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4649 - val_loss: 0.4675\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4674 - val_loss: 0.4701\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4645 - val_loss: 0.4710\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4632 - val_loss: 0.4709\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4642 - val_loss: 0.4717\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4658 - val_loss: 0.4697\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4632 - val_loss: 0.4684\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4640 - val_loss: 0.4676\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 3s 10ms/step - loss: 1.1589 - val_loss: 0.3710\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5294 - val_loss: 0.3779\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4994 - val_loss: 0.3463\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4687 - val_loss: 0.3456\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4645 - val_loss: 0.3543\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4666 - val_loss: 0.3392\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4533 - val_loss: 0.4185\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4479 - val_loss: 0.3492\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4501 - val_loss: 0.3623\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4449 - val_loss: 0.3512\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4422 - val_loss: 0.3548\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4369 - val_loss: 0.3486\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4297 - val_loss: 0.3441\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4374 - val_loss: 0.3608\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4320 - val_loss: 0.3360\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4319 - val_loss: 0.4026\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4247 - val_loss: 0.3414\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4224 - val_loss: 0.3381\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4221 - val_loss: 0.3359\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4215 - val_loss: 0.3351\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4209 - val_loss: 0.3406\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4153 - val_loss: 0.3370\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4194 - val_loss: 0.3353\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4159 - val_loss: 0.3442\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4178 - val_loss: 0.3781\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4155 - val_loss: 0.3415\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4147 - val_loss: 0.3351\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4088 - val_loss: 0.3355\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4096 - val_loss: 0.3422\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4091 - val_loss: 0.3390\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4060 - val_loss: 0.3373\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4062 - val_loss: 0.3406\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4047 - val_loss: 0.3397\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4029 - val_loss: 0.3357\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4067 - val_loss: 0.3358\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4035 - val_loss: 0.3351\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4052 - val_loss: 0.3493\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4008 - val_loss: 0.3415\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4006 - val_loss: 0.3602\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3989 - val_loss: 0.3415\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3982 - val_loss: 0.3399\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3996 - val_loss: 0.3355\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3972 - val_loss: 0.3370\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3963 - val_loss: 0.3348\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4013 - val_loss: 0.3357\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3912 - val_loss: 0.3486\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3932 - val_loss: 0.3354\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3901 - val_loss: 0.3350\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3911 - val_loss: 0.3352\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3917 - val_loss: 0.3364\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3887 - val_loss: 0.3352\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3902 - val_loss: 0.3348\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3873 - val_loss: 0.3373\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3878 - val_loss: 0.3378\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3861 - val_loss: 0.3391\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3873 - val_loss: 0.3387\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3875 - val_loss: 0.3383\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3888 - val_loss: 0.3352\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3836 - val_loss: 0.3368\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3837 - val_loss: 0.3348\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3840 - val_loss: 0.3351\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3812 - val_loss: 0.3349\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3813 - val_loss: 0.3350\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3809 - val_loss: 0.3356\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3801 - val_loss: 0.3365\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.3781 - val_loss: 0.3393\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3783 - val_loss: 0.3353\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3801 - val_loss: 0.3348\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3780 - val_loss: 0.3361\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3792 - val_loss: 0.3357\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3772 - val_loss: 0.3348\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3780 - val_loss: 0.3348\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3779 - val_loss: 0.3349\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3759 - val_loss: 0.3360\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3761 - val_loss: 0.3441\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3766 - val_loss: 0.3348\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3760 - val_loss: 0.3353\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3731 - val_loss: 0.3373\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3745 - val_loss: 0.3351\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3743 - val_loss: 0.3363\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3731 - val_loss: 0.3348\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3732 - val_loss: 0.3356\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3718 - val_loss: 0.3370\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3724 - val_loss: 0.3350\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3712 - val_loss: 0.3351\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3705 - val_loss: 0.3362\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3709 - val_loss: 0.3350\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3711 - val_loss: 0.3364\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3715 - val_loss: 0.3348\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3699 - val_loss: 0.3348\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3694 - val_loss: 0.3359\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3714 - val_loss: 0.3349\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3689 - val_loss: 0.3350\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3686 - val_loss: 0.3434\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3684 - val_loss: 0.3350\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3669 - val_loss: 0.3428\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3679 - val_loss: 0.3348\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3695 - val_loss: 0.3349\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3650 - val_loss: 0.3361\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.3655 - val_loss: 0.3349\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 3s 10ms/step - loss: 1.1534 - val_loss: 0.4500\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.6087 - val_loss: 0.4691\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5819 - val_loss: 0.4206\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5635 - val_loss: 0.4519\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5576 - val_loss: 0.4572\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5477 - val_loss: 0.4318\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5441 - val_loss: 0.4982\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5467 - val_loss: 0.4212\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5462 - val_loss: 0.4402\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5349 - val_loss: 0.4129\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5354 - val_loss: 0.4427\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5325 - val_loss: 0.4927\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5256 - val_loss: 0.4195\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5345 - val_loss: 0.4119\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5295 - val_loss: 0.4124\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5220 - val_loss: 0.4358\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5228 - val_loss: 0.4152\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5218 - val_loss: 0.4106\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5124 - val_loss: 0.4399\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5111 - val_loss: 0.4277\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5122 - val_loss: 0.4121\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5124 - val_loss: 0.4150\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5042 - val_loss: 0.4102\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5063 - val_loss: 0.4105\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5041 - val_loss: 0.4175\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5030 - val_loss: 0.4103\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5000 - val_loss: 0.4105\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4984 - val_loss: 0.4279\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4982 - val_loss: 0.4107\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4957 - val_loss: 0.4121\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4960 - val_loss: 0.4219\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4888 - val_loss: 0.4106\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4924 - val_loss: 0.4233\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4891 - val_loss: 0.4122\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4899 - val_loss: 0.4137\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4860 - val_loss: 0.4107\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4867 - val_loss: 0.4173\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4863 - val_loss: 0.4121\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4865 - val_loss: 0.4141\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4828 - val_loss: 0.4099\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4833 - val_loss: 0.4133\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4815 - val_loss: 0.4100\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4825 - val_loss: 0.4115\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4776 - val_loss: 0.4137\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4817 - val_loss: 0.4110\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4809 - val_loss: 0.4130\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4826 - val_loss: 0.4107\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4790 - val_loss: 0.4118\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4802 - val_loss: 0.4107\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4772 - val_loss: 0.4105\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4786 - val_loss: 0.4124\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4772 - val_loss: 0.4103\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4758 - val_loss: 0.4116\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4778 - val_loss: 0.4106\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4747 - val_loss: 0.4103\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4780 - val_loss: 0.4171\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4737 - val_loss: 0.4119\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4768 - val_loss: 0.4101\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4739 - val_loss: 0.4127\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4736 - val_loss: 0.4103\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4722 - val_loss: 0.4104\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4697 - val_loss: 0.4210\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4703 - val_loss: 0.4103\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4704 - val_loss: 0.4101\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4718 - val_loss: 0.4103\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4717 - val_loss: 0.4107\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4737 - val_loss: 0.4139\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4709 - val_loss: 0.4105\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4686 - val_loss: 0.4101\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4701 - val_loss: 0.4112\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4688 - val_loss: 0.4101\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4705 - val_loss: 0.4171\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4689 - val_loss: 0.4113\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4692 - val_loss: 0.4114\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4676 - val_loss: 0.4120\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4699 - val_loss: 0.4116\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4652 - val_loss: 0.4130\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4643 - val_loss: 0.4112\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4658 - val_loss: 0.4203\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4669 - val_loss: 0.4110\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4663 - val_loss: 0.4131\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4641 - val_loss: 0.4118\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4648 - val_loss: 0.4106\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4659 - val_loss: 0.4152\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4648 - val_loss: 0.4126\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4643 - val_loss: 0.4101\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4633 - val_loss: 0.4130\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4631 - val_loss: 0.4134\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4620 - val_loss: 0.4186\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4651 - val_loss: 0.4113\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4631 - val_loss: 0.4108\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4630 - val_loss: 0.4184\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4595 - val_loss: 0.4111\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4625 - val_loss: 0.4117\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4599 - val_loss: 0.4131\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4611 - val_loss: 0.4148\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4594 - val_loss: 0.4102\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4596 - val_loss: 0.4147\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4607 - val_loss: 0.4151\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4594 - val_loss: 0.4116\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 3s 10ms/step - loss: 1.1310 - val_loss: 0.6711\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.6644 - val_loss: 0.5700\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.6489 - val_loss: 0.5376\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.6160 - val_loss: 0.5343\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.6127 - val_loss: 0.5310\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5983 - val_loss: 0.5561\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5875 - val_loss: 0.5378\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5836 - val_loss: 0.5324\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.5874 - val_loss: 0.5279\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.5803 - val_loss: 0.5499\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 0.5777 - val_loss: 0.5308\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 0.5787 - val_loss: 0.5293\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5744 - val_loss: 0.5291\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5742 - val_loss: 0.5349\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5711 - val_loss: 0.5408\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5615 - val_loss: 0.5629\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5684 - val_loss: 0.5377\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5651 - val_loss: 0.5251\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5618 - val_loss: 0.5381\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5632 - val_loss: 0.5580\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5618 - val_loss: 0.5347\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5560 - val_loss: 0.5330\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5520 - val_loss: 0.5604\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5613 - val_loss: 0.5348\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5552 - val_loss: 0.5353\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5528 - val_loss: 0.5256\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5493 - val_loss: 0.5330\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5494 - val_loss: 0.5249\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5498 - val_loss: 0.5250\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5513 - val_loss: 0.5419\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5495 - val_loss: 0.5250\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5458 - val_loss: 0.5254\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5478 - val_loss: 0.5322\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5491 - val_loss: 0.5289\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5450 - val_loss: 0.5253\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5410 - val_loss: 0.5363\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5425 - val_loss: 0.5249\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5430 - val_loss: 0.5284\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5424 - val_loss: 0.5257\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5416 - val_loss: 0.5276\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5341 - val_loss: 0.5261\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5379 - val_loss: 0.5307\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5351 - val_loss: 0.5276\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5375 - val_loss: 0.5269\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5342 - val_loss: 0.5246\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5346 - val_loss: 0.5246\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5370 - val_loss: 0.5246\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5333 - val_loss: 0.5378\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5313 - val_loss: 0.5245\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5290 - val_loss: 0.5288\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5302 - val_loss: 0.5265\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5328 - val_loss: 0.5305\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5278 - val_loss: 0.5260\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5308 - val_loss: 0.5246\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5290 - val_loss: 0.5262\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5283 - val_loss: 0.5256\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5279 - val_loss: 0.5259\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5268 - val_loss: 0.5253\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5269 - val_loss: 0.5246\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5248 - val_loss: 0.5258\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5246 - val_loss: 0.5252\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5274 - val_loss: 0.5254\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5248 - val_loss: 0.5262\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5263 - val_loss: 0.5246\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5218 - val_loss: 0.5246\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5248 - val_loss: 0.5250\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5242 - val_loss: 0.5267\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5235 - val_loss: 0.5246\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5223 - val_loss: 0.5269\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5227 - val_loss: 0.5261\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5205 - val_loss: 0.5334\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5211 - val_loss: 0.5275\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5203 - val_loss: 0.5272\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5225 - val_loss: 0.5248\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5216 - val_loss: 0.5261\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5190 - val_loss: 0.5273\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5202 - val_loss: 0.5246\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5200 - val_loss: 0.5258\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5173 - val_loss: 0.5249\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5198 - val_loss: 0.5250\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5172 - val_loss: 0.5334\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5184 - val_loss: 0.5247\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5156 - val_loss: 0.5294\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5172 - val_loss: 0.5253\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5160 - val_loss: 0.5247\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5172 - val_loss: 0.5246\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5168 - val_loss: 0.5253\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5156 - val_loss: 0.5250\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5153 - val_loss: 0.5251\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5137 - val_loss: 0.5265\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5167 - val_loss: 0.5268\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5151 - val_loss: 0.5249\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5136 - val_loss: 0.5314\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5147 - val_loss: 0.5295\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5134 - val_loss: 0.5267\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5129 - val_loss: 0.5268\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5144 - val_loss: 0.5252\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5132 - val_loss: 0.5258\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5133 - val_loss: 0.5246\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5127 - val_loss: 0.5247\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 3s 12ms/step - loss: 1.0569 - val_loss: 0.4909\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.6316 - val_loss: 0.4832\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5987 - val_loss: 0.4519\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5961 - val_loss: 0.4543\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5825 - val_loss: 0.4836\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5810 - val_loss: 0.4371\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5731 - val_loss: 0.4322\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5686 - val_loss: 0.4394\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5570 - val_loss: 0.4361\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5573 - val_loss: 0.4485\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5496 - val_loss: 0.4490\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5536 - val_loss: 0.4322\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5462 - val_loss: 0.4305\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5467 - val_loss: 0.4360\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5432 - val_loss: 0.4327\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5375 - val_loss: 0.4975\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5408 - val_loss: 0.4358\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5423 - val_loss: 0.4563\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5371 - val_loss: 0.4474\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5360 - val_loss: 0.4319\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5355 - val_loss: 0.4985\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5354 - val_loss: 0.4483\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5319 - val_loss: 0.4499\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5261 - val_loss: 0.4308\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5271 - val_loss: 0.4385\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5240 - val_loss: 0.4342\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5239 - val_loss: 0.4323\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5233 - val_loss: 0.4426\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5249 - val_loss: 0.4510\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5226 - val_loss: 0.4365\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5214 - val_loss: 0.4495\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5165 - val_loss: 0.4422\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5124 - val_loss: 0.4362\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5230 - val_loss: 0.4394\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5158 - val_loss: 0.4357\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5159 - val_loss: 0.4626\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5141 - val_loss: 0.4465\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5140 - val_loss: 0.4320\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5102 - val_loss: 0.4309\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5084 - val_loss: 0.4299\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5110 - val_loss: 0.4300\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5078 - val_loss: 0.4323\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5044 - val_loss: 0.4299\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5060 - val_loss: 0.4368\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5083 - val_loss: 0.4305\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5056 - val_loss: 0.4302\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5046 - val_loss: 0.4438\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5019 - val_loss: 0.4301\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5013 - val_loss: 0.4432\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4977 - val_loss: 0.4381\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4981 - val_loss: 0.4297\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5024 - val_loss: 0.4297\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5002 - val_loss: 0.4331\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4991 - val_loss: 0.4361\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4996 - val_loss: 0.4297\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.5002 - val_loss: 0.4298\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4999 - val_loss: 0.4507\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4999 - val_loss: 0.4421\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4957 - val_loss: 0.4323\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4969 - val_loss: 0.4302\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4938 - val_loss: 0.4329\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4949 - val_loss: 0.4383\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4946 - val_loss: 0.4297\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4956 - val_loss: 0.4383\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4947 - val_loss: 0.4343\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4930 - val_loss: 0.4408\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4940 - val_loss: 0.4346\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4918 - val_loss: 0.4315\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4917 - val_loss: 0.4362\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4925 - val_loss: 0.4299\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4912 - val_loss: 0.4333\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4910 - val_loss: 0.4332\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4903 - val_loss: 0.4298\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4899 - val_loss: 0.4342\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4922 - val_loss: 0.4330\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4884 - val_loss: 0.4341\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4882 - val_loss: 0.4300\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4882 - val_loss: 0.4299\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4873 - val_loss: 0.4334\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4887 - val_loss: 0.4347\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4859 - val_loss: 0.4421\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4875 - val_loss: 0.4348\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4871 - val_loss: 0.4316\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4882 - val_loss: 0.4302\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4871 - val_loss: 0.4298\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4867 - val_loss: 0.4335\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4855 - val_loss: 0.4348\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4877 - val_loss: 0.4306\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4842 - val_loss: 0.4362\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4858 - val_loss: 0.4314\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.4868 - val_loss: 0.4349\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4853 - val_loss: 0.4377\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4844 - val_loss: 0.4311\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4831 - val_loss: 0.4500\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4836 - val_loss: 0.4299\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4808 - val_loss: 0.4336\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4826 - val_loss: 0.4298\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4813 - val_loss: 0.4497\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4823 - val_loss: 0.4314\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 0.4834 - val_loss: 0.4310\n"
          ]
        }
      ],
      "source": [
        "lstm1 = get_model_lstm();\n",
        "lstm2 = get_model_lstm();\n",
        "lstm3 = get_model_lstm();\n",
        "lstm4 = get_model_lstm();\n",
        "lstm5 = get_model_lstm();\n",
        "lstm6 = get_model_lstm();\n",
        "\n",
        "history1=lstm1.fit(lstm_embed_train, y1_train, epochs=100,validation_data=(lstm_embed_val,y1_val))\n",
        "history2=lstm2.fit(lstm_embed_train, y2_train, epochs=100,validation_data=(lstm_embed_val,y2_val))\n",
        "history3=lstm3.fit(lstm_embed_train, y3_train, epochs=100,validation_data=(lstm_embed_val,y3_val))\n",
        "history4=lstm4.fit(lstm_embed_train, y4_train, epochs=100,validation_data=(lstm_embed_val,y4_val))\n",
        "history5=lstm5.fit(lstm_embed_train, y5_train, epochs=100,validation_data=(lstm_embed_val,y5_val))\n",
        "history6=lstm6.fit(lstm_embed_train, y6_train, epochs=100,validation_data=(lstm_embed_val,y6_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "9a3bcb08",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.536187Z",
          "iopub.status.busy": "2022-11-24T19:11:54.535625Z",
          "iopub.status.idle": "2022-11-24T19:11:54.541163Z",
          "shell.execute_reply": "2022-11-24T19:11:54.540279Z"
        },
        "papermill": {
          "duration": 0.016945,
          "end_time": "2022-11-24T19:11:54.543496",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.526551",
          "status": "completed"
        },
        "tags": [],
        "id": "9a3bcb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "eb948f06-8c35-41e3-d59b-9e553d9a178b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbe838521d0>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history3.history['loss'])\n",
        "plt.plot(history4.history['loss'])\n",
        "plt.plot(history5.history['loss'])\n",
        "plt.plot(history6.history['loss'])\n",
        "\n",
        "plt.title('LSTM model with GLoVe embedding')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['syntax','cohesion','vocabulary','phraseology','grammar','conventions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0c75fb13",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.562231Z",
          "iopub.status.busy": "2022-11-24T19:11:54.561006Z",
          "iopub.status.idle": "2022-11-24T19:11:54.733090Z",
          "shell.execute_reply": "2022-11-24T19:11:54.731964Z"
        },
        "papermill": {
          "duration": 0.184355,
          "end_time": "2022-11-24T19:11:54.735700",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.551345",
          "status": "completed"
        },
        "tags": [],
        "id": "0c75fb13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8681a33-b6d6-47fd-9b21-dcb2bd8ca771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "0.3874424309011077 0.4276214803271297 0.3331457950026255 0.4090281335777037 0.46746802225137757 0.4431713160834428\n",
            "0.41131286302389786\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "y1_pred = lstm1.predict(lstm_embed_test).flatten()\n",
        "y2_pred = lstm2.predict(lstm_embed_test).flatten()\n",
        "y3_pred = lstm3.predict(lstm_embed_test).flatten()\n",
        "y4_pred = lstm4.predict(lstm_embed_test).flatten()\n",
        "y5_pred = lstm5.predict(lstm_embed_test).flatten()\n",
        "y6_pred = lstm6.predict(lstm_embed_test).flatten()\n",
        "\n",
        "\n",
        "y1_pred = [round(r,1) for r in y1_pred]\n",
        "y2_pred = [round(r,1) for r in y2_pred]\n",
        "y3_pred = [round(r,1) for r in y3_pred]\n",
        "y4_pred = [round(r,1) for r in y4_pred]\n",
        "y5_pred = [round(r,1) for r in y5_pred]\n",
        "y6_pred = [round(r,1) for r in y6_pred]\n",
        "\n",
        "\n",
        "values1 = mean_squared_error(y1_pred, y1_test)\n",
        "values2 = mean_squared_error(y2_pred, y2_test)\n",
        "values3 = mean_squared_error(y3_pred, y3_test)\n",
        "values4 = mean_squared_error(y4_pred, y4_test)\n",
        "values5 = mean_squared_error(y5_pred, y5_test)\n",
        "values6 = mean_squared_error(y6_pred, y6_test)\n",
        "\n",
        "\n",
        "print(values1,values2,values3,values4,values5,values6)\n",
        "l=[(values1,values2,values3,values4,values5,values6)]\n",
        "print(mean(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8a294974",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:11:54.753792Z",
          "iopub.status.busy": "2022-11-24T19:11:54.753360Z",
          "iopub.status.idle": "2022-11-24T19:12:06.445265Z",
          "shell.execute_reply": "2022-11-24T19:12:06.443802Z"
        },
        "papermill": {
          "duration": 11.704929,
          "end_time": "2022-11-24T19:12:06.448887",
          "exception": false,
          "start_time": "2022-11-24T19:11:54.743958",
          "status": "completed"
        },
        "tags": [],
        "id": "8a294974",
        "outputId": "0309c791-a993-4b2d-8f82-7777b6a6c8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3092114236999148\n"
          ]
        }
      ],
      "source": [
        "rf1 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
        "rf2= RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
        "rf3 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
        "rf4 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
        "rf5 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
        "rf6 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
        "\n",
        "rf1.fit(features_array_train, y1_train)\n",
        "rf2.fit(features_array_train, y2_train)\n",
        "rf3.fit(features_array_train, y3_train)\n",
        "rf4.fit(features_array_train, y4_train)\n",
        "rf5.fit(features_array_train, y5_train)\n",
        "rf6.fit(features_array_train, y6_train)\n",
        "\n",
        "\n",
        "\n",
        "y1_pred = rf1.predict(features_array_test)\n",
        "y2_pred = rf2.predict(features_array_test)\n",
        "y3_pred = rf3.predict(features_array_test)\n",
        "y4_pred = rf4.predict(features_array_test)\n",
        "y5_pred = rf5.predict(features_array_test)\n",
        "y6_pred = rf6.predict(features_array_test)\n",
        "\n",
        "y1_pred = [round(r,1) for r in y1_pred]\n",
        "y2_pred = [round(r,1) for r in y2_pred]\n",
        "y3_pred = [round(r,1) for r in y3_pred]\n",
        "y4_pred = [round(r,1) for r in y4_pred]\n",
        "y5_pred = [round(r,1) for r in y5_pred]\n",
        "y6_pred = [round(r,1) for r in y6_pred]\n",
        "\n",
        "\n",
        "values1 = mean_squared_error(y1_pred, y1_test)\n",
        "values2 = mean_squared_error(y2_pred, y2_test)\n",
        "values3 = mean_squared_error(y3_pred, y3_test)\n",
        "values4 = mean_squared_error(y4_pred, y4_test)\n",
        "values5 = mean_squared_error(y5_pred, y5_test)\n",
        "values6 = mean_squared_error(y6_pred, y6_test)\n",
        "\n",
        "\n",
        "l=[(values1,values2,values3,values4,values5,values6)]\n",
        "print(mean(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "815057a2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:12:06.468072Z",
          "iopub.status.busy": "2022-11-24T19:12:06.467621Z",
          "iopub.status.idle": "2022-11-24T19:12:08.539131Z",
          "shell.execute_reply": "2022-11-24T19:12:08.537867Z"
        },
        "papermill": {
          "duration": 2.083591,
          "end_time": "2022-11-24T19:12:08.541708",
          "exception": false,
          "start_time": "2022-11-24T19:12:06.458117",
          "status": "completed"
        },
        "tags": [],
        "id": "815057a2"
      },
      "outputs": [],
      "source": [
        "clf1 = SVR(C=10.0, epsilon=0.01)\n",
        "clf2 = SVR(C=10.0, epsilon=0.01)\n",
        "clf3 = SVR(C=10.0, epsilon=0.01)\n",
        "clf4 = SVR(C=10.0, epsilon=0.01)\n",
        "clf5 = SVR(C=10.0, epsilon=0.01)\n",
        "clf6 = SVR(C=10.0, epsilon=0.01)\n",
        "\n",
        "clf1.fit(features_array_train, y1_train)\n",
        "clf2.fit(features_array_train, y2_train)\n",
        "clf3.fit(features_array_train, y3_train)\n",
        "clf4.fit(features_array_train, y4_train)\n",
        "clf5.fit(features_array_train, y5_train)\n",
        "clf6.fit(features_array_train, y6_train)\n",
        "\n",
        "\n",
        "y1_pred = clf1.predict(features_array_test)\n",
        "y2_pred = clf2.predict(features_array_test)\n",
        "y3_pred = clf3.predict(features_array_test)\n",
        "y4_pred = clf4.predict(features_array_test)\n",
        "y5_pred = clf5.predict(features_array_test)\n",
        "y6_pred = clf6.predict(features_array_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "073c648a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:12:08.561139Z",
          "iopub.status.busy": "2022-11-24T19:12:08.560174Z",
          "iopub.status.idle": "2022-11-24T19:12:10.631788Z",
          "shell.execute_reply": "2022-11-24T19:12:10.630720Z"
        },
        "papermill": {
          "duration": 2.084113,
          "end_time": "2022-11-24T19:12:10.634487",
          "exception": false,
          "start_time": "2022-11-24T19:12:08.550374",
          "status": "completed"
        },
        "tags": [],
        "id": "073c648a",
        "outputId": "514bffd0-8c17-4a63-dbfb-8a2779177501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3092114236999148\n"
          ]
        }
      ],
      "source": [
        "y1_pred = [round(r,1) for r in y1_pred]\n",
        "y2_pred = [round(r,1) for r in y2_pred]\n",
        "y3_pred = [round(r,1) for r in y3_pred]\n",
        "y4_pred = [round(r,1) for r in y4_pred]\n",
        "y5_pred = [round(r,1) for r in y5_pred]\n",
        "y6_pred = [round(r,1) for r in y6_pred]\n",
        "\n",
        "\n",
        "values1 = mean_squared_error(y1_pred, y1_test)\n",
        "values2 = mean_squared_error(y2_pred, y2_test)\n",
        "values3 = mean_squared_error(y3_pred, y3_test)\n",
        "values4 = mean_squared_error(y4_pred, y4_test)\n",
        "values5 = mean_squared_error(y5_pred, y5_test)\n",
        "values6 = mean_squared_error(y6_pred, y6_test)\n",
        "\n",
        "\n",
        "l=[(values1,values2,values3,values4,values5,values6)]\n",
        "print(mean(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d5774f26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:12:10.657096Z",
          "iopub.status.busy": "2022-11-24T19:12:10.655849Z",
          "iopub.status.idle": "2022-11-24T19:12:25.933667Z",
          "shell.execute_reply": "2022-11-24T19:12:25.932453Z"
        },
        "papermill": {
          "duration": 15.292614,
          "end_time": "2022-11-24T19:12:25.936222",
          "exception": false,
          "start_time": "2022-11-24T19:12:10.643608",
          "status": "completed"
        },
        "tags": [],
        "id": "d5774f26"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "86e676a3",
      "metadata": {
        "papermill": {
          "duration": 0.008232,
          "end_time": "2022-11-24T19:14:13.802876",
          "exception": false,
          "start_time": "2022-11-24T19:14:13.794644",
          "status": "completed"
        },
        "tags": [],
        "id": "86e676a3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "f6bb6c92",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:13.851421Z",
          "iopub.status.busy": "2022-11-24T19:14:13.850993Z",
          "iopub.status.idle": "2022-11-24T19:14:13.865123Z",
          "shell.execute_reply": "2022-11-24T19:14:13.863197Z"
        },
        "papermill": {
          "duration": 0.027124,
          "end_time": "2022-11-24T19:14:13.868144",
          "exception": false,
          "start_time": "2022-11-24T19:14:13.841020",
          "status": "completed"
        },
        "tags": [],
        "id": "f6bb6c92"
      },
      "outputs": [],
      "source": [
        "def get_model_nn():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(64, input_dim=300, activation='tanh'))\n",
        "#     model.add(Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
        "#     model.add(Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
        "    print(model.summary())\n",
        "    model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "33bf6748",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:13.888659Z",
          "iopub.status.busy": "2022-11-24T19:14:13.888227Z",
          "iopub.status.idle": "2022-11-24T19:14:13.982630Z",
          "shell.execute_reply": "2022-11-24T19:14:13.980787Z"
        },
        "papermill": {
          "duration": 0.110366,
          "end_time": "2022-11-24T19:14:13.987508",
          "exception": false,
          "start_time": "2022-11-24T19:14:13.877142",
          "status": "completed"
        },
        "tags": [],
        "id": "33bf6748",
        "outputId": "97fe9ad2-3c14-4ea1-e4d6-70d991da630d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_60 (Dense)            (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,529\n",
            "Trainable params: 19,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_63 (Dense)            (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,529\n",
            "Trainable params: 19,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_66 (Dense)            (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,529\n",
            "Trainable params: 19,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_69 (Dense)            (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,529\n",
            "Trainable params: 19,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_72 (Dense)            (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,529\n",
            "Trainable params: 19,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_75 (Dense)            (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,529\n",
            "Trainable params: 19,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "embedding_model1 = get_model_nn()\n",
        "embedding_model2 = get_model_nn()\n",
        "embedding_model3 = get_model_nn()\n",
        "embedding_model4= get_model_nn()\n",
        "embedding_model5 = get_model_nn()\n",
        "embedding_model6 = get_model_nn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "381a32b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:14.038417Z",
          "iopub.status.busy": "2022-11-24T19:14:14.036723Z",
          "iopub.status.idle": "2022-11-24T19:14:14.061566Z",
          "shell.execute_reply": "2022-11-24T19:14:14.060049Z"
        },
        "papermill": {
          "duration": 0.05284,
          "end_time": "2022-11-24T19:14:14.064345",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.011505",
          "status": "completed"
        },
        "tags": [],
        "id": "381a32b3",
        "outputId": "06a3ee3c-a2cf-4c46-a8b3-54c393ff8673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2947 - accuracy: 0.0029 - val_loss: 0.3231 - val_accuracy: 0.0026\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.0029 - val_loss: 0.2939 - val_accuracy: 0.0026\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.0029 - val_loss: 0.2965 - val_accuracy: 0.0026\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.0029 - val_loss: 0.3340 - val_accuracy: 0.0026\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.0029 - val_loss: 0.2918 - val_accuracy: 0.0026\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.0029 - val_loss: 0.3218 - val_accuracy: 0.0026\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.0029 - val_loss: 0.2917 - val_accuracy: 0.0026\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.0029 - val_loss: 0.2990 - val_accuracy: 0.0026\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.0029 - val_loss: 0.2913 - val_accuracy: 0.0026\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.0029 - val_loss: 0.2930 - val_accuracy: 0.0026\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.0029 - val_loss: 0.3527 - val_accuracy: 0.0026\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.0029 - val_loss: 0.2968 - val_accuracy: 0.0026\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.0029 - val_loss: 0.3011 - val_accuracy: 0.0026\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.0029 - val_loss: 0.3022 - val_accuracy: 0.0026\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.0029 - val_loss: 0.2948 - val_accuracy: 0.0026\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.0029 - val_loss: 0.2931 - val_accuracy: 0.0026\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.0029 - val_loss: 0.3140 - val_accuracy: 0.0026\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.0029 - val_loss: 0.3157 - val_accuracy: 0.0026\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.0029 - val_loss: 0.2972 - val_accuracy: 0.0026\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.0029 - val_loss: 0.2960 - val_accuracy: 0.0026\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2790 - accuracy: 0.0029 - val_loss: 0.2993 - val_accuracy: 0.0026\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.0029 - val_loss: 0.2968 - val_accuracy: 0.0026\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.0029 - val_loss: 0.3063 - val_accuracy: 0.0026\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.0029 - val_loss: 0.2972 - val_accuracy: 0.0026\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.0029 - val_loss: 0.3159 - val_accuracy: 0.0026\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.0029 - val_loss: 0.2965 - val_accuracy: 0.0026\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.0029 - val_loss: 0.3732 - val_accuracy: 0.0026\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.0029 - val_loss: 0.2991 - val_accuracy: 0.0026\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.0029 - val_loss: 0.3203 - val_accuracy: 0.0026\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.0029 - val_loss: 0.2993 - val_accuracy: 0.0026\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.0029 - val_loss: 0.3120 - val_accuracy: 0.0026\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.0029 - val_loss: 0.3048 - val_accuracy: 0.0026\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.0029 - val_loss: 0.2972 - val_accuracy: 0.0026\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.0029 - val_loss: 0.3164 - val_accuracy: 0.0026\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.0029 - val_loss: 0.3177 - val_accuracy: 0.0026\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.0029 - val_loss: 0.3103 - val_accuracy: 0.0026\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.0029 - val_loss: 0.3119 - val_accuracy: 0.0026\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.0029 - val_loss: 0.3362 - val_accuracy: 0.0026\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.0029 - val_loss: 0.3281 - val_accuracy: 0.0026\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2679 - accuracy: 0.0029 - val_loss: 0.3412 - val_accuracy: 0.0026\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.0029 - val_loss: 0.3086 - val_accuracy: 0.0026\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.0029 - val_loss: 0.3120 - val_accuracy: 0.0026\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.0029 - val_loss: 0.3079 - val_accuracy: 0.0026\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.0029 - val_loss: 0.3242 - val_accuracy: 0.0026\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.0029 - val_loss: 0.3021 - val_accuracy: 0.0026\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.0029 - val_loss: 0.3108 - val_accuracy: 0.0026\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2650 - accuracy: 0.0029 - val_loss: 0.3019 - val_accuracy: 0.0026\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2641 - accuracy: 0.0029 - val_loss: 0.3033 - val_accuracy: 0.0026\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2623 - accuracy: 0.0029 - val_loss: 0.3078 - val_accuracy: 0.0026\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2642 - accuracy: 0.0029 - val_loss: 0.3057 - val_accuracy: 0.0026\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.0029 - val_loss: 0.3104 - val_accuracy: 0.0026\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2666 - accuracy: 0.0029 - val_loss: 0.3044 - val_accuracy: 0.0026\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2634 - accuracy: 0.0029 - val_loss: 0.3222 - val_accuracy: 0.0026\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.0029 - val_loss: 0.3580 - val_accuracy: 0.0026\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 0s 4ms/step - loss: 0.2591 - accuracy: 0.0029 - val_loss: 0.3187 - val_accuracy: 0.0026\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.0029 - val_loss: 0.3059 - val_accuracy: 0.0026\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.0029 - val_loss: 0.3227 - val_accuracy: 0.0026\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2601 - accuracy: 0.0029 - val_loss: 0.3065 - val_accuracy: 0.0026\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2573 - accuracy: 0.0029 - val_loss: 0.3158 - val_accuracy: 0.0026\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.0029 - val_loss: 0.3085 - val_accuracy: 0.0026\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2596 - accuracy: 0.0029 - val_loss: 0.3096 - val_accuracy: 0.0026\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.0029 - val_loss: 0.3082 - val_accuracy: 0.0026\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2597 - accuracy: 0.0029 - val_loss: 0.3123 - val_accuracy: 0.0026\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.0029 - val_loss: 0.3164 - val_accuracy: 0.0026\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.0029 - val_loss: 0.3127 - val_accuracy: 0.0026\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.0029 - val_loss: 0.3093 - val_accuracy: 0.0026\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.0029 - val_loss: 0.3314 - val_accuracy: 0.0026\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.0029 - val_loss: 0.3173 - val_accuracy: 0.0026\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.0029 - val_loss: 0.3525 - val_accuracy: 0.0026\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.0029 - val_loss: 0.3102 - val_accuracy: 0.0026\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.0029 - val_loss: 0.3148 - val_accuracy: 0.0026\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.0029 - val_loss: 0.3105 - val_accuracy: 0.0026\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.0029 - val_loss: 0.3137 - val_accuracy: 0.0026\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.0029 - val_loss: 0.3160 - val_accuracy: 0.0026\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.0029 - val_loss: 0.3096 - val_accuracy: 0.0026\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.0029 - val_loss: 0.3692 - val_accuracy: 0.0026\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.0029 - val_loss: 0.3292 - val_accuracy: 0.0026\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.0029 - val_loss: 0.3208 - val_accuracy: 0.0026\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.0029 - val_loss: 0.3124 - val_accuracy: 0.0026\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.0029 - val_loss: 0.3167 - val_accuracy: 0.0026\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.0029 - val_loss: 0.3498 - val_accuracy: 0.0026\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.0029 - val_loss: 0.3821 - val_accuracy: 0.0026\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.0029 - val_loss: 0.3155 - val_accuracy: 0.0026\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.0029 - val_loss: 0.3974 - val_accuracy: 0.0026\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.0029 - val_loss: 0.3493 - val_accuracy: 0.0026\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.0029 - val_loss: 0.3203 - val_accuracy: 0.0026\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.0029 - val_loss: 0.3182 - val_accuracy: 0.0026\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.0029 - val_loss: 0.3424 - val_accuracy: 0.0026\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.0029 - val_loss: 0.3157 - val_accuracy: 0.0026\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.0029 - val_loss: 0.3229 - val_accuracy: 0.0026\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.0029 - val_loss: 0.3341 - val_accuracy: 0.0026\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.0029 - val_loss: 0.3406 - val_accuracy: 0.0026\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.0029 - val_loss: 0.3805 - val_accuracy: 0.0026\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.0029 - val_loss: 0.3154 - val_accuracy: 0.0026\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.0029 - val_loss: 0.3159 - val_accuracy: 0.0026\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.0029 - val_loss: 0.3283 - val_accuracy: 0.0026\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.0029 - val_loss: 0.3151 - val_accuracy: 0.0026\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.0029 - val_loss: 0.3172 - val_accuracy: 0.0026\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.0029 - val_loss: 0.3199 - val_accuracy: 0.0026\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.0029 - val_loss: 0.3879 - val_accuracy: 0.0026\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.0022 - val_loss: 0.3219 - val_accuracy: 0.0026\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3118 - accuracy: 0.0022 - val_loss: 0.3222 - val_accuracy: 0.0026\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.0022 - val_loss: 0.3262 - val_accuracy: 0.0026\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.0022 - val_loss: 0.3163 - val_accuracy: 0.0026\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.0022 - val_loss: 0.3268 - val_accuracy: 0.0026\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.0022 - val_loss: 0.3315 - val_accuracy: 0.0026\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.0022 - val_loss: 0.3165 - val_accuracy: 0.0026\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3028 - accuracy: 0.0022 - val_loss: 0.3123 - val_accuracy: 0.0026\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.0022 - val_loss: 0.3178 - val_accuracy: 0.0026\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.0022 - val_loss: 0.3087 - val_accuracy: 0.0026\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.0022 - val_loss: 0.3310 - val_accuracy: 0.0026\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3007 - accuracy: 0.0022 - val_loss: 0.3342 - val_accuracy: 0.0026\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2992 - accuracy: 0.0022 - val_loss: 0.3093 - val_accuracy: 0.0026\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.0022 - val_loss: 0.3170 - val_accuracy: 0.0026\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.0022 - val_loss: 0.3067 - val_accuracy: 0.0026\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.0022 - val_loss: 0.3099 - val_accuracy: 0.0026\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.0022 - val_loss: 0.3452 - val_accuracy: 0.0026\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.0022 - val_loss: 0.3063 - val_accuracy: 0.0026\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.0022 - val_loss: 0.3084 - val_accuracy: 0.0026\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.0022 - val_loss: 0.3160 - val_accuracy: 0.0026\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.0022 - val_loss: 0.3130 - val_accuracy: 0.0026\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.0022 - val_loss: 0.3246 - val_accuracy: 0.0026\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.0022 - val_loss: 0.3066 - val_accuracy: 0.0026\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.0022 - val_loss: 0.3192 - val_accuracy: 0.0026\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.0022 - val_loss: 0.3061 - val_accuracy: 0.0026\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.0022 - val_loss: 0.3086 - val_accuracy: 0.0026\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.0022 - val_loss: 0.3298 - val_accuracy: 0.0026\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.0022 - val_loss: 0.3042 - val_accuracy: 0.0026\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.0022 - val_loss: 0.3049 - val_accuracy: 0.0026\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.0022 - val_loss: 0.3243 - val_accuracy: 0.0026\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.0022 - val_loss: 0.3221 - val_accuracy: 0.0026\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.0022 - val_loss: 0.3045 - val_accuracy: 0.0026\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.0022 - val_loss: 0.3039 - val_accuracy: 0.0026\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.0022 - val_loss: 0.3058 - val_accuracy: 0.0026\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.0022 - val_loss: 0.3484 - val_accuracy: 0.0026\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.0022 - val_loss: 0.3048 - val_accuracy: 0.0026\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.0022 - val_loss: 0.3174 - val_accuracy: 0.0026\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.0022 - val_loss: 0.3060 - val_accuracy: 0.0026\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.0022 - val_loss: 0.3036 - val_accuracy: 0.0026\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.0022 - val_loss: 0.3135 - val_accuracy: 0.0026\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.0022 - val_loss: 0.3081 - val_accuracy: 0.0026\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.0022 - val_loss: 0.3359 - val_accuracy: 0.0026\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.0022 - val_loss: 0.3226 - val_accuracy: 0.0026\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.0022 - val_loss: 0.3391 - val_accuracy: 0.0026\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.0022 - val_loss: 0.3046 - val_accuracy: 0.0026\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.0022 - val_loss: 0.3347 - val_accuracy: 0.0026\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.0022 - val_loss: 0.3133 - val_accuracy: 0.0026\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.0022 - val_loss: 0.3051 - val_accuracy: 0.0026\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.0022 - val_loss: 0.3106 - val_accuracy: 0.0026\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.0022 - val_loss: 0.3063 - val_accuracy: 0.0026\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.0022 - val_loss: 0.3043 - val_accuracy: 0.0026\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.0022 - val_loss: 0.3057 - val_accuracy: 0.0026\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.0022 - val_loss: 0.3086 - val_accuracy: 0.0026\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.0022 - val_loss: 0.3054 - val_accuracy: 0.0026\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.0022 - val_loss: 0.3047 - val_accuracy: 0.0026\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.0022 - val_loss: 0.3112 - val_accuracy: 0.0026\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.0022 - val_loss: 0.3538 - val_accuracy: 0.0026\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.0022 - val_loss: 0.3050 - val_accuracy: 0.0026\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.0022 - val_loss: 0.3291 - val_accuracy: 0.0026\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.0022 - val_loss: 0.3040 - val_accuracy: 0.0026\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.0022 - val_loss: 0.3045 - val_accuracy: 0.0026\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.0022 - val_loss: 0.3069 - val_accuracy: 0.0026\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.0022 - val_loss: 0.3255 - val_accuracy: 0.0026\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.0022 - val_loss: 0.3053 - val_accuracy: 0.0026\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.0022 - val_loss: 0.3367 - val_accuracy: 0.0026\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.0022 - val_loss: 0.3078 - val_accuracy: 0.0026\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2818 - accuracy: 0.0022 - val_loss: 0.3644 - val_accuracy: 0.0026\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.0022 - val_loss: 0.3210 - val_accuracy: 0.0026\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.0022 - val_loss: 0.3443 - val_accuracy: 0.0026\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2815 - accuracy: 0.0022 - val_loss: 0.3198 - val_accuracy: 0.0026\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2799 - accuracy: 0.0022 - val_loss: 0.3105 - val_accuracy: 0.0026\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.0022 - val_loss: 0.3208 - val_accuracy: 0.0026\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2801 - accuracy: 0.0022 - val_loss: 0.3307 - val_accuracy: 0.0026\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.0022 - val_loss: 0.3063 - val_accuracy: 0.0026\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.0022 - val_loss: 0.3117 - val_accuracy: 0.0026\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.0022 - val_loss: 0.3580 - val_accuracy: 0.0026\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.0022 - val_loss: 0.3415 - val_accuracy: 0.0026\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2810 - accuracy: 0.0022 - val_loss: 0.3119 - val_accuracy: 0.0026\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2766 - accuracy: 0.0022 - val_loss: 0.3042 - val_accuracy: 0.0026\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.0022 - val_loss: 0.3342 - val_accuracy: 0.0026\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.0022 - val_loss: 0.3074 - val_accuracy: 0.0026\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.0022 - val_loss: 0.3049 - val_accuracy: 0.0026\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.0022 - val_loss: 0.3531 - val_accuracy: 0.0026\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.0022 - val_loss: 0.3160 - val_accuracy: 0.0026\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.0022 - val_loss: 0.3362 - val_accuracy: 0.0026\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2766 - accuracy: 0.0022 - val_loss: 0.3111 - val_accuracy: 0.0026\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.0022 - val_loss: 0.3061 - val_accuracy: 0.0026\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.0022 - val_loss: 0.3059 - val_accuracy: 0.0026\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.0022 - val_loss: 0.3409 - val_accuracy: 0.0026\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.0022 - val_loss: 0.3055 - val_accuracy: 0.0026\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.0022 - val_loss: 0.3104 - val_accuracy: 0.0026\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.0022 - val_loss: 0.3293 - val_accuracy: 0.0026\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.0022 - val_loss: 0.3039 - val_accuracy: 0.0026\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.0022 - val_loss: 0.3035 - val_accuracy: 0.0026\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.0022 - val_loss: 0.3113 - val_accuracy: 0.0026\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.0022 - val_loss: 0.3539 - val_accuracy: 0.0026\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.0022 - val_loss: 0.3054 - val_accuracy: 0.0026\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.0022 - val_loss: 0.3055 - val_accuracy: 0.0026\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.0022 - val_loss: 0.3382 - val_accuracy: 0.0026\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.0022 - val_loss: 0.3072 - val_accuracy: 0.0026\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.8513 - accuracy: 0.0000e+00 - val_loss: 10.6618 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.0026 - val_loss: 0.2952 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.0026 - val_loss: 0.2800 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.3022 - accuracy: 0.0026 - val_loss: 0.2703 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.0026 - val_loss: 0.2942 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2991 - accuracy: 0.0026 - val_loss: 0.2675 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2986 - accuracy: 0.0026 - val_loss: 0.2703 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2930 - accuracy: 0.0026 - val_loss: 0.2821 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.0026 - val_loss: 0.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.0026 - val_loss: 0.2776 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2952 - accuracy: 0.0026 - val_loss: 0.2678 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.0026 - val_loss: 0.2689 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.0029 - val_loss: 0.2643 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.0026 - val_loss: 0.2713 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.0026 - val_loss: 0.2806 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.0026 - val_loss: 0.2637 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.0026 - val_loss: 0.3117 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.0029 - val_loss: 0.2750 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.0029 - val_loss: 0.2839 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2807 - accuracy: 0.0029 - val_loss: 0.2704 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.0029 - val_loss: 0.2641 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.0029 - val_loss: 0.3247 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.0029 - val_loss: 0.2735 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.0029 - val_loss: 0.2694 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.0029 - val_loss: 0.2738 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.0029 - val_loss: 0.2782 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.0029 - val_loss: 0.2745 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.0029 - val_loss: 0.2834 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2750 - accuracy: 0.0029 - val_loss: 0.2761 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.0029 - val_loss: 0.2738 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.0029 - val_loss: 0.2851 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.0029 - val_loss: 0.3416 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.0029 - val_loss: 0.3544 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.0029 - val_loss: 0.2690 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.0029 - val_loss: 0.2684 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.0029 - val_loss: 0.2747 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.0029 - val_loss: 0.2700 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.0029 - val_loss: 0.2675 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.0029 - val_loss: 0.2711 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.0029 - val_loss: 0.2699 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2719 - accuracy: 0.0029 - val_loss: 0.2911 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.0029 - val_loss: 0.2795 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.0029 - val_loss: 0.2698 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.0029 - val_loss: 0.2789 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.0029 - val_loss: 0.2819 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.0029 - val_loss: 0.2722 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.0029 - val_loss: 0.3219 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.0029 - val_loss: 0.2846 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.0029 - val_loss: 0.4036 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.0029 - val_loss: 0.2736 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.0029 - val_loss: 0.2738 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.0029 - val_loss: 0.3110 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.0029 - val_loss: 0.2819 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.0029 - val_loss: 0.2747 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.0029 - val_loss: 0.2916 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.0029 - val_loss: 0.2732 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.0029 - val_loss: 0.2794 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.0029 - val_loss: 0.2755 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.0029 - val_loss: 0.3189 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.0029 - val_loss: 0.2822 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.0029 - val_loss: 0.2775 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.0029 - val_loss: 0.3320 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.0029 - val_loss: 0.2845 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.0029 - val_loss: 0.2759 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.0029 - val_loss: 0.2947 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.0029 - val_loss: 0.2882 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.0029 - val_loss: 0.2853 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.0029 - val_loss: 0.2834 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.0029 - val_loss: 0.2817 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.0029 - val_loss: 0.2802 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.0029 - val_loss: 0.2886 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.0029 - val_loss: 0.2879 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.0029 - val_loss: 0.2818 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.0029 - val_loss: 0.3361 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.0029 - val_loss: 0.3021 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.0029 - val_loss: 0.3217 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2555 - accuracy: 0.0029 - val_loss: 0.3045 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.0029 - val_loss: 0.2963 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.0029 - val_loss: 0.2846 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.0029 - val_loss: 0.2899 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.0029 - val_loss: 0.2847 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.0029 - val_loss: 0.2845 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.0029 - val_loss: 0.2899 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.0029 - val_loss: 0.3154 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2551 - accuracy: 0.0029 - val_loss: 0.2858 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2575 - accuracy: 0.0029 - val_loss: 0.2869 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.0029 - val_loss: 0.3143 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.0029 - val_loss: 0.3033 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2555 - accuracy: 0.0029 - val_loss: 0.2859 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.0029 - val_loss: 0.3029 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.0029 - val_loss: 0.2847 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.0029 - val_loss: 0.2860 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.0029 - val_loss: 0.2891 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.0029 - val_loss: 0.2893 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.0029 - val_loss: 0.3004 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.0029 - val_loss: 0.2892 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.0029 - val_loss: 0.2900 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.0029 - val_loss: 0.2900 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.0029 - val_loss: 0.3022 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.0029 - val_loss: 0.3222 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.0029 - val_loss: 0.2880 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 9.7458 - accuracy: 0.0000e+00 - val_loss: 9.6400 - val_accuracy: 0.0000e+00\n",
            "Epoch 1/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "98/98 [==============================] - 0s 3ms/step - loss: 10.0063 - accuracy: 0.0000e+00 - val_loss: 9.5761 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "history11=embedding_model1.fit(features_array_train, y1_train, epochs=100,validation_data=(features_array_val,y1_val))\n",
        "history22=embedding_model2.fit(features_array_train, y2_train, epochs=100,validation_data=(features_array_val,y2_val))\n",
        "history33=embedding_model3.fit(features_array_train, y3_train, epochs=100,validation_data=(features_array_val,y3_val))\n",
        "history44=embedding_model4.fit(features_array_train, y4_train, epochs=100,validation_data=(features_array_val,y4_val))\n",
        "history55=embedding_model5.fit(features_array_train, y5_train, epochs=100,validation_data=(features_array_val,y5_val))\n",
        "history66=embedding_model6.fit(features_array_train, y6_train, epochs=100,validation_data=(features_array_val,y6_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "23639b66",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:14.086516Z",
          "iopub.status.busy": "2022-11-24T19:14:14.085438Z",
          "iopub.status.idle": "2022-11-24T19:14:14.109675Z",
          "shell.execute_reply": "2022-11-24T19:14:14.107792Z"
        },
        "papermill": {
          "duration": 0.038384,
          "end_time": "2022-11-24T19:14:14.112224",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.073840",
          "status": "completed"
        },
        "tags": [],
        "id": "23639b66",
        "outputId": "ac1f9367-b039-46df-e8ef-0a5e675d36df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "0.3254688802176064\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(history11.history['loss'])\n",
        "plt.plot(history22.history['loss'])\n",
        "plt.plot(history33.history['loss'])\n",
        "plt.plot(history44.history['loss'])\n",
        "plt.plot(history55.history['loss'])\n",
        "plt.plot(history66.history['loss'])\n",
        "\n",
        "plt.title('model NN')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "\n",
        "\n",
        "y1_pred = embedding_model1.predict(features_array_test).flatten()\n",
        "y2_pred = embedding_model1.predict(features_array_test).flatten()\n",
        "y3_pred = embedding_model1.predict(features_array_test).flatten()\n",
        "y4_pred = embedding_model1.predict(features_array_test).flatten()\n",
        "y5_pred = embedding_model1.predict(features_array_test).flatten()\n",
        "y6_pred = embedding_model1.predict(features_array_test).flatten()\n",
        "\n",
        "\n",
        "\n",
        "y1_pred = [round(r,1) for r in y1_pred]\n",
        "y2_pred = [round(r,1) for r in y2_pred]\n",
        "y3_pred = [round(r,1) for r in y3_pred]\n",
        "y4_pred = [round(r,1) for r in y4_pred]\n",
        "y5_pred = [round(r,1) for r in y5_pred]\n",
        "y6_pred = [round(r,1) for r in y6_pred]\n",
        "\n",
        "\n",
        "values1 = mean_squared_error(y1_pred, y1_test)\n",
        "values2 = mean_squared_error(y2_pred, y2_test)\n",
        "values3 = mean_squared_error(y3_pred, y3_test)\n",
        "values4 = mean_squared_error(y4_pred, y4_test)\n",
        "values5 = mean_squared_error(y5_pred, y5_test)\n",
        "values6 = mean_squared_error(y6_pred, y6_test)\n",
        "\n",
        "\n",
        "l=[(values1,values2,values3,values4,values5,values6)]\n",
        "print(mean(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109526cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:14.133662Z",
          "iopub.status.busy": "2022-11-24T19:14:14.133223Z",
          "iopub.status.idle": "2022-11-24T19:14:14.143954Z",
          "shell.execute_reply": "2022-11-24T19:14:14.142537Z"
        },
        "papermill": {
          "duration": 0.025557,
          "end_time": "2022-11-24T19:14:14.147831",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.122274",
          "status": "completed"
        },
        "tags": [],
        "id": "109526cf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea68311",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:14.170536Z",
          "iopub.status.busy": "2022-11-24T19:14:14.170123Z",
          "iopub.status.idle": "2022-11-24T19:14:14.183448Z",
          "shell.execute_reply": "2022-11-24T19:14:14.182241Z"
        },
        "papermill": {
          "duration": 0.027079,
          "end_time": "2022-11-24T19:14:14.186065",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.158986",
          "status": "completed"
        },
        "tags": [],
        "id": "7ea68311"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36158125",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:14.206928Z",
          "iopub.status.busy": "2022-11-24T19:14:14.206508Z",
          "iopub.status.idle": "2022-11-24T19:14:14.217222Z",
          "shell.execute_reply": "2022-11-24T19:14:14.215578Z"
        },
        "papermill": {
          "duration": 0.024178,
          "end_time": "2022-11-24T19:14:14.219957",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.195779",
          "status": "completed"
        },
        "tags": [],
        "id": "36158125"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "842572fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T19:14:14.242159Z",
          "iopub.status.busy": "2022-11-24T19:14:14.240833Z",
          "iopub.status.idle": "2022-11-24T19:14:14.256839Z",
          "shell.execute_reply": "2022-11-24T19:14:14.255844Z"
        },
        "papermill": {
          "duration": 0.029823,
          "end_time": "2022-11-24T19:14:14.259367",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.229544",
          "status": "completed"
        },
        "tags": [],
        "id": "842572fb",
        "outputId": "f46c6b53-9c63-4e2f-f374-73509a1c51ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>syntax</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id  syntax  cohesion  vocabulary  phraseology  grammar  \\\n",
              "0  0000C359D63E     3.0       3.0         3.2          3.0      2.7   \n",
              "1  000BAD50D026     2.9       3.0         3.1          3.0      2.9   \n",
              "2  00367BB2546B     3.1       3.1         3.2          3.1      3.0   \n",
              "\n",
              "   conventions  \n",
              "0          2.9  \n",
              "1          3.0  \n",
              "2          3.1  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d667782",
      "metadata": {
        "papermill": {
          "duration": 0.009139,
          "end_time": "2022-11-24T19:14:14.278485",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.269346",
          "status": "completed"
        },
        "tags": [],
        "id": "5d667782"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ada8e0",
      "metadata": {
        "papermill": {
          "duration": 0.009712,
          "end_time": "2022-11-24T19:14:14.297680",
          "exception": false,
          "start_time": "2022-11-24T19:14:14.287968",
          "status": "completed"
        },
        "tags": [],
        "id": "a9ada8e0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 182.352202,
      "end_time": "2022-11-24T19:14:17.299255",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-11-24T19:11:14.947053",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}